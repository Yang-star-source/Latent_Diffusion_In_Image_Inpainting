{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZDWlnkgES8b"
      },
      "source": [
        "# Define VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IsiXEp1nEReM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,dropout_prob = 0.0):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.norm1 = nn.GroupNorm(32,in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.norm2 = nn.GroupNorm(32,out_channels)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + self.shortcut(x1)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.norm = nn.GroupNorm(32,in_channels)\n",
        "\n",
        "        self.to_q = nn.Linear(in_channels,in_channels)\n",
        "        self.to_k = nn.Linear(in_channels,in_channels)\n",
        "        self.to_v = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "        self.to_out = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        B,C,H,W = x.shape\n",
        "        x = self.norm(x)\n",
        "        x = x.view(B,C,-1).permute(0,2,1)\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(x)\n",
        "        v = self.to_v(x)\n",
        "\n",
        "        attn = torch.bmm(q,k.permute(0,2,1)) # batch matrix multiplication\n",
        "        attn = attn * (C**(-0.5))  # sqrt(dk)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = torch.bmm(attn,v)\n",
        "\n",
        "        out = self.to_out(attn)\n",
        "        out = out.permute(0,2,1).view(B,C,H,W)\n",
        "\n",
        "        return out + residual\n",
        "\n",
        "class MidBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super(MidBlock, self).__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,in_channels)\n",
        "        self.attn1 = AttentionBlock(in_channels)\n",
        "        self.res2 = ResNetBlock(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.attn1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,has_attn=False):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "        self.down = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=2,padding=1)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,in_channels=3,out_channels=4):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.inp = nn.Conv2d(in_channels,64,kernel_size=3,padding=1)\n",
        "        self.down_block = nn.Sequential(\n",
        "            DownBlock(64,128),\n",
        "            DownBlock(128,256),\n",
        "            DownBlock(256,512,has_attn = True)\n",
        "        )\n",
        "        self.bottle = MidBlock(512)\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,512),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(512,out_channels*2,kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self,x):\n",
        "        mean , log_var = torch.chunk(x,2,dim=1)\n",
        "        log_var = torch.clamp(log_var, -30.0, 20.0)\n",
        "        D_kl = 0.5 *(torch.exp(log_var) + mean**2 - log_var - 1)\n",
        "        D_kl = torch.sum(D_kl,dim=[1,2,3]).mean() # Mean is for batch dimention\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mean + eps*std\n",
        "        return z,D_kl\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inp(x)\n",
        "        x = self.down_block(x)\n",
        "        x = self.bottle(x)\n",
        "        x = self.out(x)\n",
        "        z,D_kl = self.reparameterize(x)\n",
        "        return z,D_kl\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,has_attn=False):\n",
        "        super().__init__()\n",
        "        self.res1 = ResNetBlock(in_channels,out_channels)\n",
        "        self.res2 = ResNetBlock(out_channels,out_channels)\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2), # nearest mode by default\n",
        "            nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn(x)\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,in_channels=4,out_channels=3):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.inp = nn.Conv2d(in_channels,512,kernel_size=3,padding=1)\n",
        "        self.bottle = MidBlock(512)\n",
        "        self.up_block = nn.Sequential(\n",
        "            UpBlock(512,256,has_attn=True),\n",
        "            UpBlock(256,128),\n",
        "            UpBlock(128,64)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,64),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inp(x)\n",
        "        x = self.bottle(x)\n",
        "        x = self.up_block(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "class PatchGan(nn.Module):\n",
        "    def __init__(self,in_channels=3):\n",
        "        super(PatchGan, self).__init__()\n",
        "        self.model=nn.Sequential(\n",
        "            nn.Conv2d(in_channels,64,kernel_size=3,stride=2,padding=1),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,128),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(128,256,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,256),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(256,512,kernel_size=3,stride=2,padding=1),\n",
        "            nn.GroupNorm(32,512),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(512,1,kernel_size=3,stride=1,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,in_channels=3,out_channels=4):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder(in_channels,out_channels)\n",
        "        self.decoder = Decoder(out_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        z,D_kl = self.encoder(x)\n",
        "        x = self.decoder(z)\n",
        "        return x,D_kl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_time_embedding(time , dim):\n",
        "    if not torch.is_tensor(time):\n",
        "        time = torch.tensor(time)\n",
        "\n",
        "    device=time.device\n",
        "    if time.ndim == 0:\n",
        "        time = time.unsqueeze(0).unsqueeze(1)\n",
        "    else: # This will be execute in training since t shape is (B)\n",
        "        time = time.unsqueeze(1)\n",
        "        # (B) -> (B,1)\n",
        "\n",
        "    # important to specify device\n",
        "    i=torch.arange(dim//2,device=device).float()\n",
        "    obj = (time)/(10000**(2*i/dim))\n",
        "    return torch.cat([torch.sin(obj),torch.cos(obj)],dim=1)\n",
        "\n",
        "class time_embedding(nn.Module):\n",
        "    def __init__(self,dim):\n",
        "        super().__init__()\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Linear(dim,dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim,dim)\n",
        "        )\n",
        "\n",
        "    def forward(self,X):\n",
        "        return self.net(X)\n",
        "\n",
        "class DiffusionResNetBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,dropout_prob = 0.0 ):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.GroupNorm(32,in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.norm2 = nn.GroupNorm(32,out_channels)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "        # purpose of this projection is to match channel dim , before adding to x\n",
        "        self.time_proj = nn.Linear(time_emb_dim,out_channels)\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self,x,time_emb):\n",
        "        x1 = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # (B,C) -> (B,C,1,1)\n",
        "        # why silu ?\n",
        "        emb = self.time_proj(self.silu(time_emb))\n",
        "        x = x + emb[:, :, None, None]\n",
        "\n",
        "        x = self.norm2(x)\n",
        "        x = self.silu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.conv2(x)\n",
        "        return x + self.shortcut(x1)\n",
        "\n",
        "class DiffusionAttentionBlock(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "        super().__init__()\n",
        "        self.norm = nn.GroupNorm(32,in_channels)\n",
        "\n",
        "        self.to_q = nn.Linear(in_channels,in_channels)\n",
        "        self.to_k = nn.Linear(in_channels,in_channels)\n",
        "        self.to_v = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "        self.to_out = nn.Linear(in_channels,in_channels)\n",
        "\n",
        "    def forward(self,x):\n",
        "        residual = x\n",
        "        B,C,H,W = x.shape\n",
        "        x = self.norm(x)\n",
        "        x = x.view(B,C,-1).permute(0,2,1)\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(x)\n",
        "        v = self.to_v(x)\n",
        "\n",
        "        attn = torch.bmm(q,k.permute(0,2,1)) # batch matrix multiplication\n",
        "        attn = attn * (C**(-0.5))  # sqrt(dk)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = torch.bmm(attn,v)\n",
        "\n",
        "        out = self.to_out(attn)\n",
        "        out = out.permute(0,2,1).view(B,C,H,W)\n",
        "\n",
        "        return out + residual\n",
        "\n",
        "class DiffusionDownBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,has_attn=False):\n",
        "        super().__init__()\n",
        "        self.res1 = DiffusionResNetBlock(in_channels,out_channels,time_emb_dim)\n",
        "        self.res2 = DiffusionResNetBlock(out_channels,out_channels,time_emb_dim)\n",
        "        self.down = nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=2,padding=1)\n",
        "        if has_attn:\n",
        "            self.attn = DiffusionAttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x,time_emb):\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.res2(x,time_emb)\n",
        "        x = self.attn(x)\n",
        "\n",
        "        skip_connection = x\n",
        "\n",
        "        x = self.down(x)\n",
        "        return x , skip_connection\n",
        "\n",
        "class DiffusionMidBlock(nn.Module):\n",
        "    def __init__(self,in_channels,time_emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.res1 = DiffusionResNetBlock(in_channels,in_channels,time_emb_dim)\n",
        "        self.attn = DiffusionAttentionBlock(in_channels)\n",
        "        self.res2 = DiffusionResNetBlock(in_channels,in_channels,time_emb_dim)\n",
        "\n",
        "    def forward(self,x,time_emb):\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.attn(x)\n",
        "        x = self.res2(x,time_emb)\n",
        "        return x\n",
        "\n",
        "class DiffusionUpBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,time_emb_dim,has_attn=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels,in_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "        # Why input *2 ? Because we have to concatenate the channels\n",
        "        self.res1 = DiffusionResNetBlock(in_channels*2,out_channels,time_emb_dim)\n",
        "        self.res2 = DiffusionResNetBlock(out_channels,out_channels,time_emb_dim)\n",
        "        if has_attn:\n",
        "            self.attn = DiffusionAttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "    def forward(self,x,skip_connection,time_emb):\n",
        "        x = self.up(x)\n",
        "\n",
        "        # cancatenate at channels dimension\n",
        "        x = torch.cat([x,skip_connection],dim=1)\n",
        "        x = self.res1(x,time_emb)\n",
        "        x = self.res2(x,time_emb)\n",
        "        x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "class DiffusionUnet(nn.Module):\n",
        "    def __init__(self,in_channels=4,out_channels=4,time_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.time_dim = time_dim\n",
        "        self.time_embedding = time_embedding(time_dim)\n",
        "\n",
        "        self.init_conv = nn.Conv2d(in_channels,64,kernel_size=3,padding=1)\n",
        "\n",
        "        self.down1 = DiffusionDownBlock(64,64,time_dim)\n",
        "        self.down2 = DiffusionDownBlock(64,128,time_dim)\n",
        "        self.down3 = DiffusionDownBlock(128,128,time_dim,has_attn=True)\n",
        "        self.down4 = DiffusionDownBlock(128,256,time_dim,has_attn=True)\n",
        "\n",
        "        self.mid = DiffusionMidBlock(256,time_dim)\n",
        "\n",
        "        self.up1 = DiffusionUpBlock(256,128,time_dim,has_attn=True)\n",
        "        self.up2 = DiffusionUpBlock(128,128,time_dim,has_attn=True)\n",
        "        self.up3 = DiffusionUpBlock(128,64,time_dim)\n",
        "        self.up4 = DiffusionUpBlock(64,64,time_dim)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.GroupNorm(32,64),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(64,out_channels,kernel_size=3,padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x,t):\n",
        "        t = raw_time_embedding(t,self.time_dim)\n",
        "        emb = self.time_embedding(t)\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        x1 , skip1 = self.down1(x,emb)\n",
        "        x2 , skip2 = self.down2(x1,emb)\n",
        "        x3 , skip3 = self.down3(x2,emb)\n",
        "        x4 , skip4 = self.down4(x3,emb)\n",
        "\n",
        "        x = self.mid(x4,emb)\n",
        "\n",
        "        x = self.up1(x,skip4,emb)\n",
        "        x = self.up2(x,skip3,emb)\n",
        "        x = self.up3(x,skip2,emb)\n",
        "        x = self.up4(x,skip1,emb)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pnmxDu5PpYu-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mask(batch_size, channels, height, width,device):\n",
        "    \"\"\"\n",
        "    Generates a random box mask.\n",
        "    1 = Keep the pixel\n",
        "    0 = Drop the pixel (The Hole)\n",
        "    \"\"\"\n",
        "\n",
        "    mask = torch.ones((batch_size, 1, height, width), device=device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "\n",
        "        mask[i, :, 12:22,12:22] = 0.0\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "byRKD_WXM_7t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1 , mask on latent"
      ],
      "metadata": {
        "id": "IAQb-r2sdQKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from torch.utils.data import Dataset,DataLoader,Subset\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "VAE_CHECKPOINT = \"/content/drive/MyDrive/VAE_Training/trained_weight/checkpoint.pth\"\n",
        "UNET_PATH = \"/content/drive/MyDrive/VAE_Training/trained_weight/unet_epoch_500.pth\"\n",
        "OUTPUT_FOLDER = \"/content/drive/MyDrive/VAE_Training/Diffusion_Model/image_inpainting\"\n",
        "LATENTS_PATH = \"/content/drive/MyDrive/VAE_Training/LATENTS\"\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "vae = VAE().to(DEVICE) # Ensure your VAE class is defined above!\n",
        "checkpoint = torch.load(VAE_CHECKPOINT, map_location=DEVICE)\n",
        "vae.load_state_dict(checkpoint[\"vae_state_dict\"])\n",
        "vae.eval()\n",
        "\n",
        "unet = DiffusionUnet().to(DEVICE)\n",
        "unet.load_state_dict(torch.load(UNET_PATH))\n",
        "unet.eval()\n",
        "\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "num_timesteps = 1000\n",
        "\n",
        "betas = torch.linspace(beta_start, beta_end, num_timesteps,device=DEVICE)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "class LatentDataset(Dataset):\n",
        "    def __init__(self,root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.latent_files = [f for f in os.listdir(self.root_dir) if f.endswith('.pt')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.latent_files)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        latent_path = os.path.join(self.root_dir,self.latent_files[idx])\n",
        "        latent = torch.load(latent_path)\n",
        "        return latent\n",
        "\n",
        "dataset = LatentDataset(LATENTS_PATH)\n",
        "print(len(dataset))\n",
        "dataset = Subset(dataset,range(1))\n",
        "dataloader = DataLoader(dataset,batch_size=1,shuffle=True)\n",
        "orig_latent = next(iter(dataloader)).to(DEVICE)\n",
        "\n",
        "mask = generate_mask(1,4,32,32,DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    xt = torch.randn_like(orig_latent)\n",
        "\n",
        "    for t in reversed(range(1000)):\n",
        "        t_tensor = torch.ones(1,device=DEVICE).long() * t\n",
        "\n",
        "        alpha = alphas[t]\n",
        "        alpha_cumprod = alphas_cumprod[t]\n",
        "        beta = betas[t]\n",
        "        sqrt_alpha_cumprod = sqrt_alphas_cumprod[t]\n",
        "        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alphas_cumprod[t]\n",
        "\n",
        "        if t >0:\n",
        "            noise1 = torch.randn_like(orig_latent)\n",
        "            noise2 = torch.randn_like(orig_latent)\n",
        "        else:\n",
        "            noise1 = torch.zeros_like(orig_latent)\n",
        "            noise2 = torch.zeros_like(orig_latent)\n",
        "\n",
        "        noise_pred = unet(xt,t_tensor)\n",
        "\n",
        "        xt_minus_one = (1 / torch.sqrt(alpha)) * (xt - ((1 - alpha) / (torch.sqrt(1 - alpha_cumprod))) * noise_pred) + torch.sqrt(beta) * noise1\n",
        "\n",
        "        if t > 0:\n",
        "            xt_minus_one = xt_minus_one * (1-mask) + (sqrt_alphas_cumprod[t-1]*orig_latent + sqrt_one_minus_alphas_cumprod[t-1]*noise2) * (mask)\n",
        "\n",
        "\n",
        "        else:\n",
        "            xt_minus_one = xt_minus_one * (1-mask) + orig_latent * (mask)\n",
        "\n",
        "        xt = xt_minus_one\n",
        "\n",
        "with torch.no_grad():\n",
        "    original = vae.decoder(orig_latent)\n",
        "    reconstructed = vae.decoder(xt)\n",
        "\n",
        "    # Un-normalize (-1,1 -> 0,1)\n",
        "    reconstructed = reconstructed * 0.5 + 0.5\n",
        "    reconstructed = torch.clamp(reconstructed, 0, 1)\n",
        "\n",
        "    original = original * 0.5 + 0.5\n",
        "    original = torch.clamp(original, 0, 1)\n",
        "\n",
        "    # (2,3,256,256)\n",
        "    comebine = torch.cat([original,reconstructed],dim=0)\n",
        "    save_image(comebine, f\"{OUTPUT_FOLDER}/sample_inpainting4.png\", nrow=2)\n",
        "    print(f\"Saved to {OUTPUT_FOLDER}/sample_inpainting4.png\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2V3Kd9yQqfMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e97d218-de73-4388-a350-18d41b7f53d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5558\n",
            "Saved to /content/drive/MyDrive/VAE_Training/Diffusion_Model/image_inpainting/sample_inpainting4.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT , built UI\n",
        "The above result is sucessful , but we face 2 limitation\n",
        "\n",
        "- Our mask is on latent , not image itself\n",
        "- Our mask is square , not brush\n",
        "\n",
        "Solution:\n",
        "- we need a solution to resize the mask from 256 to 32\n",
        "- we need a UI brusher to build mask (Ask LLM)"
      ],
      "metadata": {
        "id": "4OiHgDJoDjk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio==3.48.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uk12CAqzMa-d",
        "outputId": "e06c85d9-6d0b-4c61-daaa-86fc5ad28f37"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio==3.48.0 in /usr/local/lib/python3.12/dist-packages (3.48.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==0.6.1 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (0.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (0.36.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (3.10.0)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (10.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (6.0.3)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (2.32.4)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (0.38.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio==3.48.0) (11.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==0.6.1->gradio==3.48.0) (2025.3.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==3.48.0) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==3.48.0) (2.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.14.0->gradio==3.48.0) (3.20.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.14.0->gradio==3.48.0) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.14.0->gradio==3.48.0) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.48.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.48.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.48.0) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.48.0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.48.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==3.48.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==3.48.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio==3.48.0) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.48.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.48.0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.48.0) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio==3.48.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio==3.48.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio==3.48.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.0->gradio==3.48.0) (2025.11.12)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.14.0->gradio==3.48.0) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.14.0->gradio==3.48.0) (0.16.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==3.48.0) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==3.48.0) (0.0.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->gradio==3.48.0) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->gradio==3.48.0) (1.0.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.48.0) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.48.0) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "VAE_CHECKPOINT = \"/content/drive/MyDrive/VAE_Training/trained_weight/checkpoint.pth\"\n",
        "UNET_PATH = \"/content/drive/MyDrive/VAE_Training/trained_weight/unet_epoch_500.pth\"\n",
        "\n",
        "vae = VAE().to(DEVICE) # Ensure your VAE class is defined above!\n",
        "checkpoint = torch.load(VAE_CHECKPOINT, map_location=DEVICE)\n",
        "vae.load_state_dict(checkpoint[\"vae_state_dict\"])\n",
        "vae.eval()\n",
        "\n",
        "unet = DiffusionUnet().to(DEVICE)\n",
        "unet.load_state_dict(torch.load(UNET_PATH))\n",
        "unet.eval()\n",
        "\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "num_timesteps = 1000\n",
        "\n",
        "betas = torch.linspace(beta_start, beta_end, num_timesteps,device=DEVICE)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "#                  |      SHAPE    |  TYPE    | RANGE\n",
        "# -------------------------------------------------\n",
        "#  Gradio Return   |   (256,256,3) |  uint8   | 0-255\n",
        "#  To Tensor       |   (3,256,256) |  float32 | 0.0-1.0\n",
        "def inpainting(input_dictionary):\n",
        "    image = input_dictionary[\"image\"]\n",
        "    mask = input_dictionary[\"mask\"]\n",
        "\n",
        "    img_pil = Image.fromarray(image).convert(\"RGB\")\n",
        "    mask_pil = Image.fromarray(mask)\n",
        "\n",
        "    # Batch dimention\n",
        "    img_tensor = img_transform(img_pil).unsqueeze(0).to(DEVICE)\n",
        "    mask_tensor = mask_transform(mask_pil).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    mask = F.interpolate(mask_tensor,size=(32,32),mode=\"nearest\")\n",
        "\n",
        "    # Adjust if terbalik\n",
        "    mask =( mask < 0.5).float()\n",
        "    mask = mask[:,0,:,:]\n",
        "    print(f\"Mask shape : {mask.shape}\")\n",
        "\n",
        "    orig_latent = vae.encoder(img_tensor)[0]\n",
        "    print(f\"Latent shape : {orig_latent.shape}\")\n",
        "\n",
        "    print(\"Start Diffusion Process\")\n",
        "    with torch.no_grad():\n",
        "\n",
        "        xt = torch.randn_like(orig_latent)\n",
        "\n",
        "        for t in reversed(range(1000)):\n",
        "            t_tensor = torch.ones(1,device=DEVICE).long() * t\n",
        "\n",
        "            alpha = alphas[t]\n",
        "            alpha_cumprod = alphas_cumprod[t]\n",
        "            beta = betas[t]\n",
        "            sqrt_alpha_cumprod = sqrt_alphas_cumprod[t]\n",
        "            sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alphas_cumprod[t]\n",
        "\n",
        "            if t >0:\n",
        "                noise1 = torch.randn_like(orig_latent)\n",
        "                noise2 = torch.randn_like(orig_latent)\n",
        "            else:\n",
        "                noise1 = torch.zeros_like(orig_latent)\n",
        "                noise2 = torch.zeros_like(orig_latent)\n",
        "\n",
        "            noise_pred = unet(xt,t_tensor)\n",
        "\n",
        "            xt_minus_one = (1 / torch.sqrt(alpha)) * (xt - ((1 - alpha) / (torch.sqrt(1 - alpha_cumprod))) * noise_pred) + torch.sqrt(beta) * noise1\n",
        "\n",
        "            if t > 0:\n",
        "                xt_minus_one = xt_minus_one * (1-mask) + (sqrt_alphas_cumprod[t-1]*orig_latent + sqrt_one_minus_alphas_cumprod[t-1]*noise2) * (mask)\n",
        "\n",
        "\n",
        "            else:\n",
        "                xt_minus_one = xt_minus_one * (1-mask) + orig_latent * (mask)\n",
        "\n",
        "            xt = xt_minus_one\n",
        "\n",
        "    with torch.no_grad():\n",
        "        reconstructed = vae.decoder(xt)\n",
        "\n",
        "        # Un-normalize (-1,1 -> 0,1)\n",
        "        reconstructed = reconstructed * 0.5 + 0.5\n",
        "        reconstructed = torch.clamp(reconstructed, 0, 1)\n",
        "\n",
        "        # Convert Tensor [1, C, H, W] -> Numpy [H, W, C] [0, 255] for Gradio\n",
        "        out_img = reconstructed.squeeze(0).permute(1,2,0).cpu().numpy()\n",
        "        out_img = (out_img * 255).astype(np.uint8)\n",
        "\n",
        "    return out_img\n",
        "\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=inpainting,\n",
        "    inputs = gr.Image(tool=\"sketch\" , type=\"numpy\", label=\"Upload & Draw Mask\"),\n",
        "    outputs= gr.Image(label=\"Inpainting Result\"),\n",
        "    title = \"Diffusion Inpainting Demo\",\n",
        "    description=\"Upload an image, draw over the area you want to remove, and click Submit.\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "xcmCoDOdEI9H",
        "outputId": "fcb84f09-ca84-45e2-f347-b8f772260d34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "IMPORTANT: You are using gradio version 3.48.0, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://38ffb5df5a406ec17b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://38ffb5df5a406ec17b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask shape : torch.Size([1, 32, 32])\n",
            "Latent shape : torch.Size([1, 4, 32, 32])\n",
            "Start Diffusion Process\n",
            "Mask shape : torch.Size([1, 32, 32])\n",
            "Latent shape : torch.Size([1, 4, 32, 32])\n",
            "Start Diffusion Process\n",
            "Mask shape : torch.Size([1, 32, 32])\n",
            "Latent shape : torch.Size([1, 4, 32, 32])\n",
            "Start Diffusion Process\n",
            "Mask shape : torch.Size([1, 32, 32])\n",
            "Latent shape : torch.Size([1, 4, 32, 32])\n",
            "Start Diffusion Process\n",
            "Mask shape : torch.Size([1, 32, 32])\n",
            "Latent shape : torch.Size([1, 4, 32, 32])\n",
            "Start Diffusion Process\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://38ffb5df5a406ec17b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}